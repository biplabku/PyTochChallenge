{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Neural Networks\n",
    "# including helper functionalities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "## helper file\n",
    "\n",
    "def test_network(net, trainloader):\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # Create Variables for the inputs and targets\n",
    "    inputs = Variable(images)\n",
    "    targets = Variable(images)\n",
    "\n",
    "    # Clear the gradients from all Variables\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass, then backward pass, then update weights\n",
    "    output = net.forward(inputs)\n",
    "    loss = criterion(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def view_recon(img, recon):\n",
    "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
    "        reconstruction also a PyTorch Tensor\n",
    "    '''\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "    axes[0].imshow(img.numpy().squeeze())\n",
    "    axes[1].imshow(recon.data.numpy().squeeze())\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box-forced')\n",
    "\n",
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "### end of helper file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Networks\n",
    "The network we built prior to this isnot that smart. It doesnt know anything about the handwritten digits. Neural networks with non linear activations works like universal function approximators. \n",
    "Our neural networks function maps input to the output. For example handwritten digits to class probabilities. \n",
    "\n",
    "The power of neural networks is that we can train them to approximate this functions, and basically any fucntion given enough data and compute time.\n",
    "\n",
    "How does it work ?\n",
    "At first the network has no idea about the data. It doesnot the function mapping the inputs to the outputs.  We would train the networks by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "How to find these paramters ?\n",
    "To do that we need to know how badly the network is predicting the real outputs.  For this we calculate a  loss function, a measure of our predicting error.  Example - The mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "loss = 1/2n(input - output)^2\n",
    "\n",
    "input = true labels\n",
    "output = predicted labels\n",
    "\n",
    "By minimizing the loss wrt the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy.\n",
    "We find this minimum using a process called gradient descent. The gradient is the slope of the loss functions and points in the direction of fastest change, To get to the minimum in the least amount of time we want to follow the gradient downward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "For single layer networks, gradient descent is straightforward to implement. However for deeper networks its more complicated for deeper, multilayer neural networks like the one we have built. \n",
    "\n",
    "It all comes down to backpropagation which is really just an application of chain rule from calculus. \n",
    "\n",
    "To train the weights  with gradient descent, we pass the gradient of the loss backwards through the network. Each  operation has some gradient between inputs and outputs. As we send the gradient backwards, we multiply the incoming gradient with gradient for the operations. \n",
    "\n",
    "W1 = W1 - alpha * dL/dW1\n",
    "\n",
    "Learning rate alpha is set such that weight update steps are small enought that the iterative method settles in a minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses in PyTorch\n",
    "Through the nn module, PyTorch provides losses such as the cross entropy loss (nn.CrossEntropyLoss). \n",
    "\n",
    "Here the loss will be assigned to criterion. \n",
    "\n",
    "With MNIST data, we are using the softmax function to predict class probabilities. With a softmax output, you want to use cross entropy as the loss. \n",
    "\n",
    "Here first define the criterio and then pass in the output of your network and the correct labels.\n",
    "\n",
    "nn.CrossEntropyLoss, combines nn.LogSoftmax() and nn.NLLLoss() in one single class. \n",
    "\n",
    "Meaning -- Pass in the raw output of our network into the loss, not the output of the softmax function. This output is the logits or the scores.  Softmax gives you the probabilities which will often be very close to zero or one but floating-points numbers cant accurately represent values near zero or one. Its usually best to avoid doing calculations with probabilities, tpically we use log-probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "                               ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/PyTorchChallenge/MNIST_dara/', download = True, train=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2913, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128, 64),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(64, 10)\n",
    "                     )\n",
    "# define the loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our Data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten the images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "\n",
    "# Calculate the loss with the logits and labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3073, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Building a feed forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128, 64),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(64, 10),\n",
    "                     nn.LogSoftmax(dim = 1)\n",
    "                     )\n",
    "# define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "Now we need to find the gradient at each step which done through backpropagation after getting the output from feedforward process.  \n",
    "\n",
    "Torch provides a module, autograd for automatically calculating the gradients of all our parameters wrt the loss. \n",
    "\n",
    "Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. \n",
    "\n",
    "How to do that ?\n",
    "\n",
    "Pytorch does exactly that by setting requires_grad = True on a tensor. you can do this at creation with the requires_grad keyword or at any time with x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0007, -0.0007, -0.0007,  ..., -0.0007, -0.0007, -0.0007],\n",
      "        [-0.0042, -0.0042, -0.0042,  ..., -0.0042, -0.0042, -0.0042],\n",
      "        [-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
      "        ...,\n",
      "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005],\n",
      "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0021,  0.0021,  0.0021],\n",
      "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005]])\n"
     ]
    }
   ],
   "source": [
    "# autograd\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also turn off gradients for a block of code with torch.no_grad()\n",
    "\n",
    "Globally -  torch.set_grad_enabled(True|False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8279,  0.7094],\n",
      "        [ 0.1064, -0.1540]], requires_grad=True)\n",
      "tensor([[0.6854, 0.5032],\n",
      "        [0.0113, 0.0237]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x119d0bfd0>\n"
     ]
    }
   ],
   "source": [
    "tr = torch.randn(2,2, requires_grad=True)\n",
    "print(tr)\n",
    "y = tr**2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd module keeps track of these operations and knows how to calculate the gradient for each one. In this way, its able to caculate the gradients for a chain of operations, wrt any one tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3059, grad_fn=<MeanBackward1>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)\n",
    "print(tr.grad)\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "How to train the network ?\n",
    "To start training, an optimizer that we will use to update the weights with the gradient. Pytorch provides optim.SGD which is stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "#optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "# lets try with one learning step before looping through all the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General process with PyTorch\n",
    "1. Make a forward pass through the network\n",
    "2. use the network output to calculate the loss\n",
    "3. Perform a backward pass through the network with loss.backward() to calculate the gradients\n",
    "4. Take a step with the optimizer to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0209,  0.0314, -0.0251,  ..., -0.0031, -0.0125,  0.0269],\n",
      "        [-0.0178, -0.0182,  0.0284,  ..., -0.0022, -0.0103, -0.0105],\n",
      "        [ 0.0103,  0.0128, -0.0061,  ..., -0.0226, -0.0184,  0.0086],\n",
      "        ...,\n",
      "        [-0.0100, -0.0303, -0.0227,  ..., -0.0175, -0.0089, -0.0342],\n",
      "        [-0.0065, -0.0303, -0.0150,  ...,  0.0221,  0.0196, -0.0090],\n",
      "        [-0.0255, -0.0233,  0.0005,  ..., -0.0003, -0.0086,  0.0345]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
      "        [-0.0041, -0.0041, -0.0041,  ..., -0.0041, -0.0041, -0.0041],\n",
      "        ...,\n",
      "        [ 0.0086,  0.0086,  0.0086,  ...,  0.0086,  0.0086,  0.0086],\n",
      "        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n",
      "        [ 0.0010,  0.0010,  0.0010,  ...,  0.0010,  0.0010,  0.0010]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0209,  0.0314, -0.0251,  ..., -0.0031, -0.0125,  0.0269],\n",
      "        [-0.0178, -0.0181,  0.0285,  ..., -0.0021, -0.0103, -0.0105],\n",
      "        [ 0.0103,  0.0128, -0.0061,  ..., -0.0226, -0.0183,  0.0087],\n",
      "        ...,\n",
      "        [-0.0101, -0.0304, -0.0228,  ..., -0.0176, -0.0090, -0.0343],\n",
      "        [-0.0065, -0.0303, -0.0150,  ...,  0.0221,  0.0196, -0.0090],\n",
      "        [-0.0255, -0.0233,  0.0005,  ..., -0.0003, -0.0086,  0.0345]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0302011175895296\n",
      "Training loss: 0.3794196746083719\n",
      "Training loss: 0.32309446377413614\n",
      "Training loss: 0.29099254774005173\n",
      "Training loss: 0.2664174638164323\n",
      "Training loss: 0.24529754484036584\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim = 1)\n",
    "                    )\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "epochs = 6\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() # parameter update step based on the current gradient\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with the network trained, we can check out its predictions\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "\n",
    "# turn of gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "    \n",
    "# output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "view_classify(img.view(1,28,28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

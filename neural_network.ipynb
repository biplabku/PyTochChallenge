{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks with PyTorch\n",
    "Deep learning networks has more number of hidden layers and hence the term \"deep\" learning. You can build one of these deep neural networks using only weights matrices. \n",
    "Pytorch has a very nice module nn that gives a nice way to efficiently build large neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all necessary packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this chapter We are going to build a fully connected network larged than the one we did\n",
    "# Identifying text in an image, in this one we will identify the letters in an image(grayscale) which consists of handwritten \n",
    "# digits. Each image is 28 x 28 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal - Identify the digits in the image using a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need the data set to work with. In this case we are going to use MNIST dataset through torchvision package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "### run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transfrom to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                               ])\n",
    "trainset = datasets.MNIST('~/.PyTorchChallenge/MNIST_data/', train=True, transform=transform, download=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainset is responsible for downloading the data set from the internet with the above transform which means datas are normalize with mean of 0.5 and standard deviation of 0.5\n",
    "\n",
    "Trainloader loads the images from the dataset with a batch size of 64 and shuffled. Batch size is the number of images we get in one iteration from the dataloader and pass through our network. when shuffle is set to true - Means the the batch shuffle with each iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12088c0b8>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example - Lets try to build a simple network for this dataset using weight matrices and matrix multiplications.\n",
    "1. Input to each layer must be one dimensional vector that can be stacked into a 2D tensor as a batch of multiple examples.\n",
    "2. Images are 28 x 28 2D tensors, so we need to convert them into 1D vectors i.e flatten them.\n",
    "3. Convert the batches of images with shape(64, 1, 28, 28) to have a shape of(64, 784) 784 means  28 times 28 which is the image size basically flattening them into 1D vectors\n",
    "4. Here we need 10 output units meaning one for each digit.  We want our network to predict the digit shown in an image. \n",
    "5. We will calculate probabilities that the image is any one digit or calss. \n",
    "6. This ends up being a discrete probability distribution over the classes or digits that tells us the most likely class for the image. \n",
    "7. In summary we need 10 output units for the 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "## solution to finding the output\n",
    "# first i have to get the images\n",
    "# images are flattened with the below command\n",
    "def activation(x):\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "## 2 methods to do that\n",
    "#inputs = images.reshape(images.shape[0] * images.shape[1], images.shape[2] * images.shape[3])\n",
    "\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "w1 = torch.randn(784, 256)\n",
    "w2 = torch.randn(256, 10)\n",
    "\n",
    "b1 = torch.randn(256)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "hidden = activation(torch.mm(inputs, w1) + b1)\n",
    "output = activation(torch.mm(output1, w2) + b2)\n",
    "\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have to define the probabilities \n",
    "We have 10 outputs for our network. We will pass in an image into the network and get out a probability distribution over the classes that tells us the likely classes the images belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "# define softmax distribution\n",
    "# note - about view -- (-1,1) ==> . column shapes is defined by 1 but -1 is automatically decided by the numerator term\n",
    "def softmax(x):\n",
    "    den_sum = torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "    \n",
    "    return torch.exp(x)/(den_sum)\n",
    "\n",
    "probabilities = softmax(output)\n",
    "print(probabilities.shape)\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building networks with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch provides a module nn that makes building networks much simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        \n",
    "        # Output Layer, 10 units - One for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # define sigmoid activation and softmax output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor throught each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "## Notes to the prior code implementation\n",
    "# class Network(nn.module):\n",
    "# Here we are inheriting from nn.module. Combined with super().__init__() this creates\n",
    "# a class that tracks the architecture and provides a lot of useful methods and attributes. \n",
    "# It is mandatory to inherit from nn.module when you are creating a class for your network\n",
    "\n",
    "# self.hidden = nn.Linear(784, 256)\n",
    "# It creates a module for a Linear transformation, xW + b with the mentioned params and assigns it to self.hidden\n",
    "# this method automatically creates the weights and bias tensors which we will use\n",
    "# in the forward method. \n",
    "# we can still access the Weights and Bias tensors once the network is created with net.hidden.weight and net.hidden.bias\n",
    "# similarly\n",
    "# self.output = nn.Linear(256, 10)\n",
    "\n",
    "# setting dim =1 in nn.Softmax(dim = 1) calculates softmax across columns\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch networks created with nn.module must have a forward method defined. It takes in a tensor x and passes it through the operations defined in the init method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the network and look at its text representation\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define network more concisely and clearly using torch.nn.functional module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # hidden layer with sigmoid function\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # output layer with softnax activation\n",
    "        x = F.softmax(self.output(x), dim = 1)\n",
    "        return x\n",
    "    \n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its not necessarily needed to use softmax for activation function. The only requirement is that for a network to approximate a non linear function, the activation functions must be non limear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784, 128)\n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        \n",
    "        return x\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0353,  0.0185, -0.0312,  ...,  0.0098, -0.0161,  0.0006],\n",
      "        [ 0.0024, -0.0043, -0.0156,  ..., -0.0161,  0.0069, -0.0098],\n",
      "        [ 0.0282,  0.0282, -0.0235,  ...,  0.0151, -0.0115,  0.0109],\n",
      "        ...,\n",
      "        [-0.0214,  0.0047, -0.0021,  ...,  0.0142,  0.0105, -0.0223],\n",
      "        [ 0.0057,  0.0070, -0.0301,  ..., -0.0231, -0.0189, -0.0099],\n",
      "        [-0.0347,  0.0259,  0.0088,  ...,  0.0304,  0.0288, -0.0231]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-6.5523e-03,  1.7662e-02,  2.1370e-02,  3.1335e-02, -2.1521e-02,\n",
      "        -5.4332e-03,  2.7082e-02, -1.1717e-02,  2.2495e-02, -2.6022e-02,\n",
      "         2.2068e-02, -1.0131e-02, -1.6754e-02, -1.1473e-02, -1.2044e-02,\n",
      "         3.1589e-02, -1.5446e-02,  6.8428e-03,  2.9614e-02, -2.3176e-02,\n",
      "        -8.0865e-04,  2.0901e-02,  1.8081e-02, -6.9467e-03,  3.4460e-02,\n",
      "         3.0847e-03,  8.1710e-03, -2.7394e-02, -3.2651e-02, -1.3705e-02,\n",
      "         3.2626e-02,  2.8949e-02, -3.4762e-02, -2.4655e-02, -1.7132e-02,\n",
      "        -3.0857e-02,  3.5186e-02,  3.1410e-02, -3.0470e-02,  1.2280e-02,\n",
      "         1.8382e-02,  3.1333e-02,  3.0851e-02, -1.3624e-02,  1.2200e-02,\n",
      "        -9.9070e-03,  2.2855e-02, -3.3137e-02, -3.2264e-03,  1.9172e-02,\n",
      "        -1.6682e-02,  1.7449e-02,  3.2299e-02,  2.4017e-02,  2.3911e-02,\n",
      "         2.3426e-02,  3.1923e-02,  1.5099e-04, -1.4034e-02, -3.8764e-03,\n",
      "         8.8950e-03,  1.7317e-02, -2.5083e-02, -1.1485e-02,  9.4328e-03,\n",
      "        -2.3307e-02, -3.4348e-02, -1.1727e-02,  2.1877e-02, -1.5627e-03,\n",
      "         3.0697e-02, -3.3014e-02,  1.2184e-02,  5.3360e-03, -3.2682e-03,\n",
      "        -2.5862e-02,  9.5431e-05,  1.9913e-02,  8.9154e-04,  2.6723e-03,\n",
      "        -3.1313e-03, -2.8099e-02, -2.0142e-02, -2.9484e-02, -6.0779e-03,\n",
      "        -2.2647e-02, -2.6681e-02,  1.9653e-02,  6.1831e-03,  1.3805e-02,\n",
      "         1.8952e-02,  3.6219e-03, -2.7480e-02, -1.8345e-02, -1.4019e-02,\n",
      "        -7.0083e-03, -3.2323e-02,  2.0127e-02, -1.0257e-02,  3.5125e-02,\n",
      "        -4.2573e-03, -1.0138e-02, -1.6895e-02,  2.8205e-02, -8.8098e-03,\n",
      "        -1.6746e-03, -2.3544e-02,  3.4304e-02, -1.6179e-03, -1.4570e-02,\n",
      "        -2.8452e-02, -1.9553e-03,  1.1050e-02, -1.3156e-03, -3.3560e-02,\n",
      "         3.1272e-02,  1.3073e-03, -1.7486e-02,  1.1161e-02,  7.9570e-04,\n",
      "        -1.5398e-02,  2.8130e-02,  2.7884e-02,  7.6121e-03, -1.7439e-02,\n",
      "         1.2957e-02, -2.4111e-02,  3.4120e-02], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and such are automatically initialized. \n",
    "# below are the ways through which we can access the values\n",
    "print(model.hidden1.weight)\n",
    "print(model.hidden1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are actually autograd Variables, so we need to get back the actual tensors with model.fc1.weight.data. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0035,  0.0031,  0.0014,  ...,  0.0109,  0.0117, -0.0106],\n",
       "        [ 0.0079, -0.0120, -0.0002,  ..., -0.0073, -0.0083, -0.0036],\n",
       "        [-0.0144,  0.0034,  0.0109,  ..., -0.0099, -0.0069, -0.0008],\n",
       "        ...,\n",
       "        [ 0.0018,  0.0104,  0.0060,  ...,  0.0054, -0.0102,  0.0048],\n",
       "        [-0.0157,  0.0043,  0.0086,  ..., -0.0247,  0.0001, -0.0066],\n",
       "        [-0.0141,  0.0099, -0.0054,  ..., -0.0094, -0.0023,  0.0121]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden1.bias.data.fill_(0)\n",
    "model.hidden1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'view_classify'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-a22ccd8c508f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'helper' has no attribute 'view_classify'"
     ]
    }
   ],
   "source": [
    "# Now we are ready with the model so we can pass an image and see what happens\n",
    "detailer = iter(trainloader)\n",
    "images, labels = detailer.next()\n",
    "\n",
    "# resize the images into a 1D vector, new shape is (batch size, color channels, image pixels)\n",
    "images.resize_(64,1,784)\n",
    "\n",
    "# forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "img = images[img_idx]\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

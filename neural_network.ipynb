{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks with PyTorch\n",
    "Deep learning networks has more number of hidden layers and hence the term \"deep\" learning. You can build one of these deep neural networks using only weights matrices. \n",
    "Pytorch has a very nice module nn that gives a nice way to efficiently build large neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all necessary packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this chapter We are going to build a fully connected network larged than the one we did\n",
    "# Identifying text in an image, in this one we will identify the letters in an image(grayscale) which consists of handwritten \n",
    "# digits. Each image is 28 x 28 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal - Identify the digits in the image using a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need the data set to work with. In this case we are going to use MNIST dataset through torchvision package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "### run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transfrom to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                               ])\n",
    "trainset = datasets.MNIST('~/.PyTorchChallenge/MNIST_data/', train=True, transform=transform, download=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainset is responsible for downloading the data set from the internet with the above transform which means datas are normalize with mean of 0.5 and standard deviation of 0.5\n",
    "\n",
    "Trainloader loads the images from the dataset with a batch size of 64 and shuffled. Batch size is the number of images we get in one iteration from the dataloader and pass through our network. when shuffle is set to true - Means the the batch shuffle with each iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12750d9b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example - Lets try to build a simple network for this dataset using weight matrices and matrix multiplications.\n",
    "1. Input to each layer must be one dimensional vector that can be stacked into a 2D tensor as a batch of multiple examples.\n",
    "2. Images are 28 x 28 2D tensors, so we need to convert them into 1D vectors i.e flatten them.\n",
    "3. Convert the batches of images with shape(64, 1, 28, 28) to have a shape of(64, 784) 784 means  28 times 28 which is the image size basically flattening them into 1D vectors\n",
    "4. Here we need 10 output units meaning one for each digit.  We want our network to predict the digit shown in an image. \n",
    "5. We will calculate probabilities that the image is any one digit or calss. \n",
    "6. This ends up being a discrete probability distribution over the classes or digits that tells us the most likely class for the image. \n",
    "7. In summary we need 10 output units for the 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "## solution to finding the output\n",
    "# first i have to get the images\n",
    "# images are flattened with the below command\n",
    "def activation(x):\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "## 2 methods to do that\n",
    "#inputs = images.reshape(images.shape[0] * images.shape[1], images.shape[2] * images.shape[3])\n",
    "\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "w1 = torch.randn(784, 256)\n",
    "w2 = torch.randn(256, 10)\n",
    "\n",
    "b1 = torch.randn(256)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "hidden = activation(torch.mm(inputs, w1) + b1)\n",
    "output = activation(torch.mm(output1, w2) + b2)\n",
    "\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have to define the probabilities \n",
    "We have 10 outputs for our network. We will pass in an image into the network and get out a probability distribution over the classes that tells us the likely classes the images belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "# define softmax distribution\n",
    "# note - about view -- (-1,1) ==> . column shapes is defined by 1 but -1 is automatically decided by the numerator term\n",
    "def softmax(x):\n",
    "    den_sum = torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "    \n",
    "    return torch.exp(x)/(den_sum)\n",
    "\n",
    "probabilities = softmax(output)\n",
    "print(probabilities.shape)\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building networks with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pytorch provides a module nn that makes building networks much simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        \n",
    "        # Output Layer, 10 units - One for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # define sigmoid activation and softmax output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor throught each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

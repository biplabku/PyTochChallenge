{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style transfer and content\n",
    "\n",
    "Applying the style of one image to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style transfer finds the style of one image and content of the other\n",
    "# and finally it tries to merge the two to create a new third image\n",
    "# objects and their arrangement are taken from the content image\n",
    "# and the colors and textures are taken from the style image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style transfer with Deep Neural Networks\n",
    "Style transfer uses the features found in the 19-layer VGG network, which is comprised of a series of convolutional and pooling layers and a few fully connected layers.\n",
    "Convolutional layers are named by stack and their order in the stack.\n",
    "Example Conv1_1 is the first convolutional layer in the first stack and conv2_1 is the second convoltuional layer in the second stack. the deepest convolutional layer in the network is conv5_4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Style and Content\n",
    "Style transfer relies on separating the content and style of an image. Given one content image and one style image, we aim to create a new target image which should contain our desired content and style components.\n",
    "\n",
    "1. Objects and their arrangement are similar to that of the content image\n",
    "2. Style colors and textures are similar to that of the style image\n",
    "\n",
    "In this notebook, we will use a pre trained VGG19 network to extract or style features from a passed in image. We will then formalize the idea of content and style losses and use those to iteratively update our target image until we get a result that we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in VGG19 (features)\n",
    "\n",
    "VGG19 is split into 2 portions:\n",
    "1. vgg19.features, which are all the convolutional and pooling layers\n",
    "2. vgg19.classifier, which are the 3 linear, classifier layers at the end.\n",
    "\n",
    "We only need features portion which we are going to load in and freeze the weights of, below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features portion of VGG19\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "# freezing all VGG parameters since we are only optimizing the target image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
